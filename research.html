<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="mediaqueries.css">
</head>
<body>
  <nav id="desktop-nav">
    <div class="logo"> <a href="./index.html">Personal Webpage</a></div>
    <div>
        <ul class="nav-links">
            <li> <a href="./index.html">About</a> </li>
            <li> <a href="#">Research</a> </li>
            <li> <a href="./teaching.html">Teaching</a> </li>
            <li> <a href="./talks.html">Talks</a> </li>
            <li> <a href="./other.html">Others</a> </li>
            <li> <a href="./index.html">Contact</a> </li>
        </ul>
    </div>
</nav>
<nav id="hamburger-nav">
  <div class="logo"> <a href="./index.html"> Personal Webpage</a></div>
  <div class="hamburger-menu">
      <div class="hamburger-icon" onclick="toggleMenu()">
          <span></span>
          <span></span>
          <span></span>
      </div>
      <div class="menu-links">
          <li> <a href="./index.html" onclick="toggleMenu()">About</a> </li>
          <li> <a href="#" onclick="toggleMenu()">Research</a> </li>
          <li> <a href="./teaching.html" onclick="toggleMenu()">Teaching</a> </li>
          <li> <a href="./talks.html" onclick="toggleMenu()">Talks</a> </li>
          <li> <a href="./other.html" onclick="toggleMenu()">Others</a> </li>
          <li> <a href="./index.html" onclick="toggleMenu()">Contact</a> </li>
      </div>
  </div>

</nav>

    <section id="experience">
      <div class="section__pic-container" style="height: 30%; width: 70%;margin-left: -4rem;padding-bottom: 2rem;">
        <img src="./assets/PASS-1.png" style="display: flex;" alt="PASS-1" />
        <img src="./assets/NewFail-1.png" style="display: flex;" alt="NewFail-1" />
        <img src="./assets/DataProfile_TolAnalysis (1).png" style="display: flex;" alt="data-profile" />
      </div>
        
        <div class="experience-details-container">
          <h3>Double Descent and Overfitting under Noisy Inputs and Distribution Shift for Linear Denoisers</h3>
          <h4>C. Kausik, K. Srivastava and R. Sonthalia, Transactions of Machine Learning Research, 2024. (<a href="https://openreview.net/pdf?id=HxfqTdLIRF" style="color: #B76C48;">Journal</a>).</h4>
          <p>Despite the importance of denoising in modern machine learning and ample empirical work on supervised denoising, its theoretical understanding is still relatively scarce. One concern about studying supervised denoising is that one might not always have noiseless training data from the test distribution. It is more reasonable to have access to noiseless training data from a different dataset than the test dataset. Motivated by this, we study supervised denoising and noisy-input regression under distribution shift. We add three considerations to increase the applicability of our theoretical insights to real-life data and modern machine learning. First, while most past theoretical work assumes that the data covariance matrix is full-rank and well-conditioned, empirical studies have shown that real-life data is approximately low-rank. Thus, we assume that our data matrices are low-rank. Second, we drop independence assumptions on our data. Third, the rise in computational power and dimensionality of data have made it important to study non-classical regimes of learning. Thus, we work in the non-classical proportional regime, where data dimension 'd' and number of samples  'N' grow as 'd/N = c + o(1)'. </p>
            <p>For this setting, we derive \rishi{data-dependent, instance specific} expressions for the test error for both denoising and noisy-input regression, and study when overfitting the noise is benign, tempered or catastrophic. We show that the test error exhibits double descent under general distribution shift, providing insights for data augmentation and the role of noise as an implicit regularizer. We also perform experiments using real-life data, where we match the theoretical predictions with under 1\% MSE error for low-rank data.</p>
        </div>
        <hr>
        <div class="experience-details-container">
          <h3>Stochastic enzyme kinetics and the quasi-steady-state reductions: Application of the slow scale linear noise approximation `a la Fenichel</h3>
          <h4>J. Eilertsen, K. Srivastava and S. Schnell, J. Math. Biol. 85, 3 (2022). (<a href="https://link.springer.com/article/10.1007/s00285-022-01768-6" style="color: #B76C48;">Journal</a>).</h4>
          <p>The linear noise approximation models the random fluctuations from the mean-field model of a chemical reaction that unfolds near the thermodynamic limit. Specifically, the fluctuations obey a linear Langevin equation up to order Ω−1/2, where Ω is the size of the chemical system (usually the volume). In the presence of disparate timescales, the linear noise approximation admits a quasi-steady-state reduction referred to as the slow scale linear noise approximation (ssLNA). Curiously, the ssLNAs reported in the literature are slightly different. The differences in the reported ssLNAs lie at the mathematical heart of the derivation. In this work, we derive the ssLNA directly from geometric singular perturbation theory and explain the origin of the different ssLNAs in the literature. Moreover, we discuss the loss of normal hyperbolicity and we extend the ssLNA derived from geometric singular perturbation theory to a non-classical singularly perturbed problem. In so doing, we disprove a commonly-accepted qualifier for the validity of stochastic quasi-steady-state approximation of the Michaelis – Menten reaction mechanism. </p>
        </div>
        <hr>
        <div class="experience-details-container">
          <h3>Applying Complex-step Derivative Approximations in Model-based Derivative-Free Optimization</h3>
          <h4>W. Hare and K. Srivastava, Pacific Journal of Optimization, 2023. (<a href="http://www.yokohamapublishers.jp/online2/pjov19-3.html" style="color: #B76C48;">Journal</a>).</h4>
          <p>We consider the problem of minimizing an objective function that is provided by an oracle. We assume that while the optimization problem seeks a real-valued solution, the oracle is capable of accepting complex-valued input and returning complex-valued output. We explore using complex-variables in order to approximate gradients and Hessians within a derivative-free optimization method. We provide several complex-variable based methods to construct approximate gradients and Hessians, then provide numerical error bounds for these methods. We apply the approximations in a Newton algorithm to numerically explore the pros and cons of each approximation technique. Results find that complex-variable based methods improve the chances finding higher accuracy solutions and allow for smaller step sizes to be applied; but, require increased cpu time that outstrips any reduction in function calls used or iterations applied. </p>
        </div>
        <hr>
        <div class="experience-details-container" style="padding-bottom: 3rem;">
          <h3>Learning Partial Differential Equations from Noisy Data using Neural Networks</h3>
          <h4>K. Srivastava, M. Ahlawat, J. Singh, and V. Kumar, J. Phys.: Conf. Ser. 1655 012075, 2020. (<a href="https://iopscience.iop.org/article/10.1088/1742-6596/1655/1/012075/meta" style="color: #B76C48;">Journal</a>).</h4>
          <p>The problem of learning partial differential equations (PDEs) from given data is investigated here. Several algorithms have been developed for PDE learning from accurate data sets. These include using sparse optimization for approximating the coefficients of candidate terms in a general PDE model. In this work, the study is extended to spatiotemporal data sets with various noise levels. We compare the performance of conventional and novel methods for denoising the data. Different architectures of neural networks are used to denoise data and approximate derivatives. These methods are numerically tested on the linear convection-diffusion equation and the nonlinear convection-diffusion equation (Burgers' equation). Results suggest that modification in the hidden units/hidden layers in the network architecture have an effect on the accuracy of approximations with a significant rate. This is a further improvement on the previously known denoising methods of finite differences, polynomial regression splines and single layer neural network. </p>
        </div>

      </section>
      <script src="script.js"></script>
</body>
</html>